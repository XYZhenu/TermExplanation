---
description: >-
  关键字: 二进制,八进制,十六进制,字节,bit,Byte,KB,MB,GB,TB,8,1024;
  ASCII,Unicode,UTF-8,UTF-16,UTF-32
---

# **字节编码**

*关键字：二进制,八进制,十六进制,字节,bit,Byte,KB,MB,GB,TB,8,1024;
  ASCII,Unicode,UTF-8,UTF-16,UTF-32*

## **英文文字的计算机表示**

计算机的电子元件被做成只能识别 **开** 和 **关** 两种状态，开代表1，关代表0。

很自然的，计算机所有的数据，所有的命令都是用**二进制**来表示。1位数据，就是1**bit\(比特\)**，只能表示1和0这2种状态。2位组合有4种状态，3位有8种状态……7位有128种状态，8位有256种状态。

最初设计计算机系统的美国人把8位\(8bit\)定义为**1Byte\(字节\)**。因为8位完全能把所有的英文单词，标点，数字，特殊字符表示完，最初的这种编码就是**ASCII** (American Standard Code for Information Interchange)。下表列举了一部分ASCII字符的表示方式。 8位二进制从0000 0000到0111 1111，对应的十进制是0到127，正好128种状态。

注意所有英文字符只用了7位，总共8位，剩下的一位呢？剩下的128种状态呢？

其实在最初的ASCII中，最后1位是用来做**奇偶校验**的，举个例子：
一个文本里面有100个字符，我怎么确定我发送的这100个字符没错呢？奇校验的做法是对100位里面的每一位，都数一下它们里面1的个数，如果个数为奇数就在最后一位写上0，如果为偶数就写上1。这样字符**A**本身是0100 0001，编码后就变成了1100 0001。偶校验正好相反，1个数为奇数最后一位就写1，否则写0。

另外还有一些系统使用了扩展ASCII码，就是把最后一位也用作字符表示上。没有奇偶校验。


| Bin\(二进制\) | Oct\(八进制\) | Dec\(十进制\) | Hex\(十六进制\) | 缩写/字符 | 解释 |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 0000 0000 | 0 | 0 | 00 | NUL\(null\) | 空字符 |
| 0000 0001 | 1 | 1 | 01 | SOH\(start of headline\) | 标题开始 |
| 0000 0010 | 2 | 2 | 02 | STX \(start of text\) | 正文开始 |
| ………… | ………… | ………… | ………… | ………… | ………… |
| 0010 1111 | 57 | 47 | 2F | / | 斜杠 |
| 0011 0000 | 60 | 48 | 30 | 0 | 数字0 |
| 0011 0001 | 61 | 49 | 31 | 1 | 数字1 |
| 0011 0010 | 62 | 50 | 32 | 2 | 数字2 |
| 0011 0011 | 63 | 51 | 33 | 3 | 数字3 |
| ………… | ………… | ………… | ………… | ………… | ………… |
| 0100 0001 | 101 | 65 | 41 | A | 大写字母A |
| 0100 0010 | 102 | 66 | 42 | B | 大写字母B |
| 0100 0011 | 103 | 67 | 43 | C | 大写字母C |
| ………… | ………… | ………… | ………… | ………… | ………… |
| 0111 1101 | 175 | 125 | 7D | } | 闭花括号 |
| 0111 1110 | 176 | 126 | 7E | ~ | 波浪号 |
| 0111 1111 | 177 | 127 | 7F | DEL (delete) | 删除 |
------

## **多进制**

根据上表，我们可以知道，**在计算机里其实所有的字符都可以用一个唯一的数字来表示**。不同于我们习惯的**十进制**是逢10进1，**二进制**是逢2进1，**八进制**是逢8进1，**十六进制**是逢16进1。如果你想，你可以逢7进1搞个七进制，也可以逢64进1搞个六十四进制。

当然阿拉伯数字只有0到9这10个数字，其他多进制基数超过10的我们可以用其他字符来表示。比如十六进制就用A-F这5个数字来分别表示10-15这几个数。

计算机用二进制是因为计算机内部电子元件天然就能做成识别0,1两种状态。

我们使用十进制是因为我们天然就有10根手指头。

我们还用十六进制存储数据是因为十六进制写起来比二进制短太多了，二进制写一段话需要一张纸而十六进制可能就只是需要写一行。

在计算机里我们习惯于使用2的倍数，毕竟计算机天然对2友好，转化为其他二的倍数更容易。比如：
>对八进制的17，我们分别把2位数字用二进制的3位扩充就好，1对应001，7对应110，组合起来就是001110。

>对十六进制的2A，我们分别把2位数字用二进制的4位扩充就好，2对应0010，A对应1010，组合起来就是00101010。

## **字节单位转换**

从上文我们知道这么一个换算关系：

```
1 bit = 0 和 1 两种状态；
1 Byte = 8 bit;

为了方便表示大量数据，我们需要更大的计量单位，一般约定每一个单位量级之间乘数是1024.
为什么不是1000？上一段说过，2天然就对计算机友好，10只是对人友好。

1024 = 2^10 = 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2 * 2

1 KB = 1024 B;
1 MB = 1024 KB;
1 GB = 1024 MB;
1 TB = 1024 GB;
1 PB = 1024 TB;

还有我们常用的网速表示方法，比如百兆宽带100Mbps，注意单位是 bps,也就是bit per second。所以真正的网速是：

100 Mbps = 100 / 8 MBps = 1.25 M/s

真正网速只有1.25MB每秒。
```

## **更多文字的计算机表示**

这世界上不是只有英文的，英文128个字符用1 Byte ASCII码表示就可以了，但是汉字，拉丁文，日文，韩文等等各种文字加起来就多了，汉字就多达10万左右。要是每一个国家都自己编一个字符表那么互联网上大家就不能沟通了，因为在汉语中一个表示文字的数字在另一种语言中表示的是另外一个符号，结果就是大量的乱码。中文最初的编码就是**GBK**。

为了建立统一的编码库，解决乱码问题，出现了**Unicode**。[Unicode官网](http://www.unicode.org/)，Unicode规定了符号用哪个数字来表示，却没有规定这个数字应该如何存储。

从[Unicode汉字编码表](http://www.chi2ko.com/tool/CJK.htm)里查到，汉字**岩**的数字表示是**5CA9**，转换成二进制是**0101 1100 1010 1001**，换算成十进制就是**23721**。对于汉字**岩**来说，我们至少需要2个字节16bit来存储。字符很多，有的字符甚至需要4个字节32bit来存储。

Unicode对于ASCII是兼容的，也就是从0到127这些数字是为ASCII里面的字符预留的。字母**A**还是**0100 0001**。

这样就出现一个问题，我们怎么存储这些Unicode编码的文字，如果我们规定都用4个字节来表示，就会造成很多浪费，互联网上大部分都是只需要一个字节就能表示的英文字符，都用4个字节就意味着每存储一个英文字符就有3bit被浪费。这3bit加在互联网那巨大的信息量上浪费是惊人的。

**UTF-32**就是这么做的，用4个字节32bit来表示。**UTF-16**更进一步，用2个字节或4个字节来表示，节省了很多空间。最为广泛使用的是**UTF-8**，它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。

UTF-8的规则是：
+ 对于1 bit的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。
+ 对于需要n bit的符号，第一个bit的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。

| 字符 | Bin\(二进制\) | bit | UTF-8 | UTF-8b编码后大小 |
| :---: | :---: | :---: | :---: | :---: |
| A | 01000001 | 1 bit | 01000001 | 1 bit |
| 岩 | 01011100 10101001 | 2 bit | **1110**0101  **10**110010  **10**101001 | 3 bit |
| …… | …… | …… | **1110**xxxx **10**xxxxxx **10**xxxxxx | 3 bit |
| …… | …… | …… | **11110**xxx **10**xxxxxx **10**xxxxxx **10**xxxxxx| 4 bit |
-----

## 总结
计算机里所有数据，命令，都是以二进制存储在介质里面的，这个介质可以使内存条，移动硬盘，固态硬盘，U盘，光盘等等。存储的意思就是人为规定一些数字与现实世界中信息的对照关系，然后在计算机中按照某种编码格式存下这些数字，当需要使用的时候再读出来这些数字并且按照对照关系翻译成有意义的，人类可以识别的语言。
不管是什么编码，实质上它们就是按顺序存储了一系列的数字。UTF-8存储的也就是数字。
